{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98558e0d-b53a-412c-81a9-9bb54f749c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from tensorflow.keras.models import Model, load_model, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, Add, Lambda, Concatenate, Reshape, Flatten\n",
    "from tensorflow.keras.initializers import GlorotUniform, RandomUniform, Constant\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15cb903-d121-4c70-952d-9f60a5c352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/OneDrive - Universidad Complutense de Madrid (UCM)/Doctorado/Curriculum_Learning/JAX_MODULES-Easy_Multidigit_Addition_Decimal/'\n",
    "folder_specific = f'{folder}DECISION_MODULE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5220b5d7-767d-4616-9750-922b01af448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow no está utilizando la GPU\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay GPUs disponibles\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow está utilizando la GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow no está utilizando la GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf29484-c704-488a-b9ae-bc4b4260d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase que define la estructura del modelo\n",
    "class carry_LSTMModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        lstm_1 = nn.LSTMCell(features=16)\n",
    "        dense = nn.Dense(2)\n",
    "\n",
    "        carry1 = lstm_1.initialize_carry(jax.random.PRNGKey(0), (x.shape[0],)) \n",
    "\n",
    "        for t in range(x.shape[1]):  # Iterar sobre los pasos temporales\n",
    "            carry1, x_t = lstm_1(carry1, x[:, t])\n",
    "\n",
    "        hidden_state = carry1[0] \n",
    "        final_output = nn.softmax(dense(hidden_state))\n",
    "        return final_output\n",
    "\n",
    "class unit_LSTMModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        lstm_1 = nn.LSTMCell(features=16)\n",
    "        lstm_2 = nn.LSTMCell(features=32)\n",
    "        lstm_3 = nn.LSTMCell(features=16)\n",
    "        dense = nn.Dense(10)\n",
    "\n",
    "        carry1 = lstm_1.initialize_carry(jax.random.PRNGKey(0), (x.shape[0],))\n",
    "        carry2 = lstm_2.initialize_carry(jax.random.PRNGKey(1), (x.shape[0],))\n",
    "        carry3 = lstm_3.initialize_carry(jax.random.PRNGKey(2), (x.shape[0],))\n",
    "\n",
    "        for t in range(x.shape[1]):  # Iterar sobre los pasos temporales\n",
    "            carry1, x_t = lstm_1(carry1, x[:, t])\n",
    "            carry2, x_t = lstm_2(carry2, x_t)\n",
    "            carry3, x_t = lstm_3(carry3, x_t)\n",
    "\n",
    "        hidden_state = carry3[0]  # Estado oculto tiene forma (batch_size, 32)\n",
    "        final_output = nn.softmax(dense(hidden_state))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c8a971-a305-48fe-94a9-40be4a3f8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los módulos preentrenados (unit_module y carry_module)\n",
    "with open(f'{folder}Modules/unit_module_JAX.pkl', \"rb\") as f:\n",
    "    unit_model = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}Modules/carry_module_JAX.pkl', \"rb\") as f:\n",
    "    carry_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e640dcb3-ef42-4589-8409-d946e4f66dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las parejas desde el archivo\n",
    "with open(f\"{folder}train_couples_stimuli.txt\", \"r\") as file:\n",
    "    train_couples = eval(file.read())\n",
    "\n",
    "with open(f\"{folder}stimuli.txt\", \"r\") as file:\n",
    "    test_couples = eval(file.read())\n",
    "\n",
    "with open(f\"{folder}test_dataset.txt\", \"r\") as file:\n",
    "    test_dataset = eval(file.read())\n",
    "\n",
    "# Calcular frecuencias basadas en exp(-(a+b)/N) y normalizar\n",
    "probabilities = np.array([np.exp(-(a + b) / 100) for a, b in train_couples])\n",
    "probabilities /= probabilities.sum()  # Normalizar para convertir en una distribución de probabilidad\n",
    "\n",
    "small_problem_size = [pair for pair in train_couples if (pair[0] + pair[1]) < 40]\n",
    "medium_problem_size = [pair for pair in train_couples if 40 <= (pair[0] + pair[1]) <= 60]\n",
    "large_problem_size = [pair for pair in train_couples if (pair[0] + pair[1]) > 60]\n",
    "\n",
    "def generate_test_dataset():\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for a, b in test_dataset:\n",
    "        a_dec = a // 10  # Decena del primer número\n",
    "        a_unit = a % 10  # Unidad del primer número\n",
    "        b_dec = b // 10  # Decena del segundo número\n",
    "        b_unit = b % 10  # Unidad del segundo número\n",
    "\n",
    "        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "\n",
    "        sum_units = (a_unit + b_unit) % 10\n",
    "        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "        y_data.append([sum_dec, sum_units])  # Salida\n",
    "    \n",
    "    return jnp.array(x_data), jnp.array(y_data)\n",
    "\n",
    "# Función para generar el dataset de entrenamiento dinámicamente\n",
    "def generate_train_dataset(train_couples, size_epoch):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # Seleccionar parejas aleatoriamente según las probabilidades\n",
    "    selected_indices = np.random.choice(len(train_couples), size=size_epoch, p=probabilities)\n",
    "    selected_couples = [train_couples[i] for i in selected_indices]\n",
    "    #selected_indices = np.random.choice(len(large_problem_size), size=size_epoch, replace=True)\n",
    "    #selected_couples = [large_problem_size[i] for i in selected_indices]\n",
    "\n",
    "    for a, b in selected_couples:\n",
    "        a_dec = a // 10  # Decena del primer número\n",
    "        a_unit = a % 10  # Unidad del primer número\n",
    "        b_dec = b // 10  # Decena del segundo número\n",
    "        b_unit = b % 10  # Unidad del segundo número\n",
    "\n",
    "        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "\n",
    "        sum_units = (a_unit + b_unit) % 10\n",
    "        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "        y_data.append([sum_dec, sum_units])  # Salida\n",
    "    \n",
    "    return jnp.array(x_data), jnp.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50c8854-39ee-48ff-be9f-9e3f462a9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar el dataset de entrenamiento dinámicamente\n",
    "#def generate_train_dataset(train_couples, size_epoch):\n",
    "#    x_data = []\n",
    "#    y_data = []\n",
    "#    \n",
    "#    # Calcular el número de muestras a seleccionar de cada clase\n",
    "#    total_classes = 3\n",
    "#    balanced_class_count = size_epoch // total_classes\n",
    "#    remaining = size_epoch - total_classes * balanced_class_count\n",
    "#\n",
    "#    # Seleccionar las muestras balanceadas de cada clase\n",
    "#    balanced_small_indices = np.random.choice(len(small_problem_size), size=balanced_class_count, replace=True)\n",
    "#    balanced_small = [small_problem_size[i] for i in balanced_small_indices]\n",
    "#    balanced_medium_indices = np.random.choice(len(medium_problem_size), size=balanced_class_count, replace=True)\n",
    "#    balanced_medium = [medium_problem_size[i] for i in balanced_medium_indices]\n",
    "#    balanced_large_indices = np.random.choice(len(large_problem_size), size=balanced_class_count, replace=True)\n",
    "#    balanced_large = [large_problem_size[i] for i in balanced_large_indices]\n",
    "#\n",
    "#    # Rellenar aleatoriamente las clases restantes\n",
    "#    remaining_classes = ['small', 'medium', 'large']\n",
    "#    remaining_classes_idx = np.random.choice(remaining_classes, size=remaining, replace=True)\n",
    "#\n",
    "#    # Rellenar las clases con los elementos restantes\n",
    "#    for class_type in remaining_classes_idx:\n",
    "#        #print(class_type)\n",
    "#        if class_type == 'small':\n",
    "#            balanced_small_choice = np.random.choice(len(small_problem_size))\n",
    "#            balanced_small.append(small_problem_size[balanced_small_choice])\n",
    "#        elif class_type == 'medium':\n",
    "#            balanced_medium_choice = np.random.choice(len(medium_problem_size))\n",
    "#            balanced_medium.append(medium_problem_size[balanced_medium_choice])\n",
    "#        else:\n",
    "#            balanced_large_choice = np.random.choice(len(large_problem_size))\n",
    "#            balanced_large.append(large_problem_size[balanced_large_choice])\n",
    "#\n",
    "#    # Seleccionar las parejas balanceadas\n",
    "#    #print(balanced_small)\n",
    "#    #print(balanced_medium)\n",
    "#    #print(balanced_large)\n",
    "#    selected_couples = np.concatenate((balanced_small, balanced_medium, balanced_large))\n",
    "#    np.random.shuffle(selected_couples)\n",
    "#\n",
    "#    for a, b in selected_couples:\n",
    "#        a_dec = a // 10  # Decena del primer número\n",
    "#        a_unit = a % 10  # Unidad del primer número\n",
    "#        b_dec = b // 10  # Decena del segundo número\n",
    "#        b_unit = b % 10  # Unidad del segundo número\n",
    "#\n",
    "#        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "#\n",
    "#        sum_units = (a_unit + b_unit) % 10\n",
    "#        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "#        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "#        y_data.append([sum_dec, sum_units])  # Salida\n",
    "#    \n",
    "#    return jnp.array(x_data), jnp.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92293af0-33a3-408d-98e3-a37a680a8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de pérdida\n",
    "def loss_fn(params, x, y):\n",
    "    y_pred_1, y_pred_2 = model(params, x)\n",
    "    return jnp.mean((y_pred_1 - y[:, 0]) ** 2) + jnp.mean((y_pred_2 - y[:, 1]) ** 2)\n",
    "    \n",
    "# Función para actualizar los parámetros\n",
    "def update_params(params, x, y, lr):\n",
    "    # Asegúrate de usar JAX para los gradientes y operaciones\n",
    "    gradients = grad(loss_fn)(params, x, y)\n",
    "    #for key, gradient in gradients.items():\n",
    "    #    print(f\"Gradiente para {key}: {gradient}\")\n",
    "    new_params = jax.tree_util.tree_map(lambda p, g: p - lr * g, params, gradients)\n",
    "    return new_params\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(params, train_couples, size_epoch, lr=0.01, epochs=100, stop=1, batch_size=0):\n",
    "    final_loss = 0\n",
    "\n",
    "    if batch_size == 0:\n",
    "        batch_size = size_epoch\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    for epoch in range(epochs): \n",
    "        x_train, y_train = generate_train_dataset(train_couples, size_epoch)\n",
    "        total_examples = x_train.shape[0]\n",
    "\n",
    "        batches_per_epoch = int(total_examples / batch_size)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            pred_count, pred_count_test, step_loss = correct_predictions_and_loss(params)  # Contamos las predicciones correctas\n",
    "            print(f\"Epoch {epoch}, Loss: {step_loss}, Correct predictions: {pred_count}, Correct predictions test: {pred_count_test}\")\n",
    "            \n",
    "        for batch in range(batches_per_epoch):         \n",
    "            x_batch = x_train[(batch * batch_size):((batch + 1) * batch_size)]\n",
    "            y_batch = y_train[(batch * batch_size):((batch + 1) * batch_size)]\n",
    "            params = update_params(params, x_batch, y_batch, lr)\n",
    "            \n",
    "        # Mostrar estadísticas cada 10 épocas\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            pred_count, pred_count_test, step_loss = correct_predictions_and_loss(params)  # Contamos las predicciones correctas\n",
    "            print(f\"Epoch {epoch}, Loss: {step_loss}, Correct predictions: {pred_count}, Correct predictions test: {pred_count_test}\")\n",
    "            \n",
    "            # Criterios de parada\n",
    "            if stop == 1 and pred_count_test == 192:\n",
    "                break\n",
    "            if stop == 0 and pred_count == 5050:\n",
    "                break\n",
    "            if step_loss >= 1000000:\n",
    "                break\n",
    "        \n",
    "    return params, step_loss\n",
    "\n",
    "def correct_predictions_and_loss(params):\n",
    "    x_test, y_test = generate_test_dataset()\n",
    "    pred_count = 0\n",
    "    pred_count_test = 0\n",
    "    total_examples = x_test.shape[0]\n",
    "    pred_tens, pred_units = model(params, x_test)   \n",
    "    loss = jnp.mean((pred_tens - y_test[:, 0]) ** 2) + jnp.mean((pred_units - y_test[:, 1]) ** 2)\n",
    "    for i in range(total_examples):\n",
    "        normalized_pred = [int(jnp.round(pred_tens[i].item())),\n",
    "                           int(jnp.round(pred_units[i].item()))]\n",
    "        \n",
    "        # Obtener los valores a y b de x_test\n",
    "        a = int(str(x_test[i, 0]) + str(x_test[i, 1]))\n",
    "        b = int(str(x_test[i, 2]) + str(x_test[i, 3]))\n",
    "        # Comparar las predicciones con las etiquetas y contar los aciertos\n",
    "        if normalized_pred[0] == y_test[i, 0] and normalized_pred[1] == y_test[i, 1]:\n",
    "            pred_count += 1\n",
    "            if (a, b) in test_couples:\n",
    "                pred_count_test += 1\n",
    "\n",
    "    return pred_count, pred_count_test, loss\n",
    "\n",
    "def correct_predictions(params):\n",
    "    x_test, y_test = generate_test_dataset()\n",
    "    pred_count = 0\n",
    "    pred_count_test = 0\n",
    "    total_examples = x_test.shape[0]\n",
    "    pred_tens, pred_units = model(params, x_test)        \n",
    "    for i in range(total_examples):\n",
    "        normalized_pred = [int(jnp.round(pred_tens[i].item())),\n",
    "                           int(jnp.round(pred_units[i].item()))]\n",
    "        \n",
    "        # Obtener los valores a y b de x_test\n",
    "        a = int(str(x_test[i, 0]) + str(x_test[i, 1]))\n",
    "        b = int(str(x_test[i, 2]) + str(x_test[i, 3]))\n",
    "        # Comparar las predicciones con las etiquetas y contar los aciertos\n",
    "        if normalized_pred[0] == y_test[i, 0] and normalized_pred[1] == y_test[i, 1]:\n",
    "            pred_count += 1\n",
    "            if (a, b) in test_couples:\n",
    "                pred_count_test += 1\n",
    "\n",
    "    return pred_count, pred_count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8df9371-de6b-4e71-b765-5d6933ee3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo dinámico en JAX\n",
    "def model(params, x):\n",
    "    carry_val = {}\n",
    "    unit_val = {}\n",
    "    \n",
    "    for i in [0,1]:\n",
    "        for j in [2,3]:\n",
    "            units_inputs = jnp.array(x[:, [i, j]])\n",
    "            units_input = units_inputs[:, None, :]\n",
    "            \n",
    "            unit_output = unit_LSTMModel().apply({'params':unit_model}, units_input)\n",
    "            carry_output = carry_LSTMModel().apply({'params': carry_model}, units_input)\n",
    "            print(carry_output)\n",
    "            unit_val[f'{i}_{j}'] = jnp.argmax(unit_output, axis=-1)\n",
    "            carry_val[f'{i}_{j}'] = jnp.argmax(carry_output, axis=-1)\n",
    "    \n",
    "    salida_1 = sum(params[f'v_0_1_{i}_{j}'] * carry_val[f'{i}_{j}'] +\n",
    "        params[f'v_1_1_{i}_{j}'] * unit_val[f'{i}_{j}']\n",
    "        for i in [0,1] for j in [2,3]\n",
    "        )\n",
    "\n",
    "    # Salida 2\n",
    "    salida_2 = sum(params[f'v_0_2_{i}_{j}'] * carry_val[f'{i}_{j}'] +\n",
    "        params[f'v_1_2_{i}_{j}'] * unit_val[f'{i}_{j}']\n",
    "        for i in [0,1] for j in [2,3]\n",
    "        )\n",
    "\n",
    "    return salida_1, salida_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad68821-8b61-473b-ba5b-dba5eee4634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer parámetros de un archivo JSON\n",
    "def load_params_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "def save_trained_model(params, filename, model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    file_path = os.path.join(model_dir, filename)\n",
    "    serializable_params = {key: value.tolist() for key, value in params.items()}\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(serializable_params, f)\n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, file, mode='w'):\n",
    "        self.file = open(file, mode)\n",
    "        self.console = sys.stdout  \n",
    "\n",
    "    def write(self, data):\n",
    "        self.console.write(data)   \n",
    "        self.file.write(data)    \n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb30455-12b9-41f9-9dfa-a993a1ac3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with epsilon = 0.1\n",
      "Loaded trainable_model_2025_01_22_12_04_33.json\n",
      "[[0.9945458  0.00545419]\n",
      " [0.9945458  0.00545419]\n",
      " [0.9945458  0.00545419]\n",
      " ...\n",
      " [0.89615583 0.10384422]\n",
      " [0.89615583 0.10384422]\n",
      " [0.89615583 0.10384422]]\n",
      "[[9.9454582e-01 5.4541896e-03]\n",
      " [9.9911195e-01 8.8799180e-04]\n",
      " [9.9941313e-01 5.8687304e-04]\n",
      " ...\n",
      " [8.9615583e-01 1.0384422e-01]\n",
      " [4.9242154e-01 5.0757843e-01]\n",
      " [8.9615583e-01 1.0384422e-01]]\n",
      "[[0.9945458  0.00545419]\n",
      " [0.9945458  0.00545419]\n",
      " [0.9945458  0.00545419]\n",
      " ...\n",
      " [0.96006286 0.03993707]\n",
      " [0.96006286 0.03993707]\n",
      " [0.89615583 0.10384422]]\n",
      "[[9.9454582e-01 5.4541896e-03]\n",
      " [9.9911195e-01 8.8799180e-04]\n",
      " [9.9941313e-01 5.8687304e-04]\n",
      " ...\n",
      " [9.6006286e-01 3.9937068e-02]\n",
      " [7.1861094e-01 2.8138903e-01]\n",
      " [8.9615583e-01 1.0384422e-01]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m tee\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m---> 38\u001b[0m     new_params, average_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable_model_jnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_couples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     pred_count, pred_count_test \u001b[38;5;241m=\u001b[39m correct_predictions(new_params)\n\u001b[0;32m     41\u001b[0m     trained_model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(params, train_couples, size_epoch, lr, epochs, stop, batch_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m batches_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(total_examples \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     pred_count, pred_count_test, step_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_predictions_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Contamos las predicciones correctas\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Correct predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Correct predictions test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_count_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batches_per_epoch):         \n",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mcorrect_predictions_and_loss\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean((pred_tens \u001b[38;5;241m-\u001b[39m y_test[:, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean((pred_units \u001b[38;5;241m-\u001b[39m y_test[:, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_examples):\n\u001b[0;32m     61\u001b[0m     normalized_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(jnp\u001b[38;5;241m.\u001b[39mround(pred_tens[i]\u001b[38;5;241m.\u001b[39mitem())),\n\u001b[1;32m---> 62\u001b[0m                        \u001b[38;5;28mint\u001b[39m(jnp\u001b[38;5;241m.\u001b[39mround(\u001b[43mpred_units\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))]\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Obtener los valores a y b de x_test\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(x_test[i, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(x_test[i, \u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:212\u001b[0m, in \u001b[0;36m_item\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_item\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array, \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mcomplex\u001b[39m:\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Copy an element of an array to a standard Python scalar and return it.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m   arr \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete_or_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis occurred in the item() method of jax.Array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, dtypes\u001b[38;5;241m.\u001b[39mextended):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Python scalar type for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\jax\\_src\\core.py:1518\u001b[0m, in \u001b[0;36mconcrete_or_error\u001b[1;34m(force, val, context)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m force(maybe_concrete)\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_type = 'AP'\n",
    "\n",
    "epsilons = [0.1]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    print(f'Starting with epsilon = {epsilon}')\n",
    "    save_dir = f\"{folder_specific}Results_models/{param_type}_{epsilon}\"\n",
    "    save_model_dir = f\"{folder_specific}Trained_models/{param_type}_{epsilon}\"\n",
    "    save_model_dir_2 = f\"{folder_specific}Super_trained_models/{param_type}_{epsilon}\"\n",
    "    folder_path = f'{folder_specific}Parameters/{param_type}_{epsilon}'\n",
    "    date_pattern = r'trainable_model_(\\d{4}_\\d{2}_\\d{2}_\\d{2}_\\d{2}_\\d{2}).json'\n",
    "    files = sorted(\n",
    "        (f for f in os.listdir(folder_path) if not f.startswith('.')),  # Filtrar archivos ocultos\n",
    "        key=lambda x: re.search(date_pattern, x).group(1) if re.search(date_pattern, x) else ''\n",
    "    )\n",
    "    \n",
    "    for filename in files:\n",
    "        match = re.search(date_pattern, filename)\n",
    "        if match:\n",
    "            current_time = match.group(1)\n",
    "        else:\n",
    "            print('Error')\n",
    "            break\n",
    "        \n",
    "        file_path = f\"{folder_path}/trainable_model_{current_time}.json\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            trainable_model = json.load(file)\n",
    "    \n",
    "        trainable_model_jnp = {key: jnp.array(value) for key, value in trainable_model.items()}\n",
    "        print(f'Loaded trainable_model_{current_time}.json')\n",
    "       \n",
    "        os.makedirs(save_dir, exist_ok=True) \n",
    "        results_file = os.path.join(save_dir, f\"Results_{current_time}.txt\") \n",
    "        tee = Tee(results_file, 'w') \n",
    "        sys.stdout = tee\n",
    "        \n",
    "        try: \n",
    "            new_params, average_loss = train_model(trainable_model_jnp, train_couples, size_epoch=1000, lr=0.01, epochs=100, stop=1, batch_size=0)\n",
    "            pred_count, pred_count_test = correct_predictions(new_params)\n",
    "\n",
    "            trained_model_filename = f\"trained_model_{current_time}.json\"\n",
    "            save_trained_model(new_params, trained_model_filename, save_model_dir)\n",
    "            print(f'Saved trained_model_{current_time}.json')\n",
    "    \n",
    "            if pred_count != 5051:\n",
    "                new_params_2, average_loss_2 = train_model(new_params, train_couples, size_epoch=1000, lr=0.01, epochs=500, stop=0, batch_size=0)\n",
    "                trained_model_filename_2 = f\"super_trained_model_{current_time}.json\"\n",
    "                save_trained_model(new_params_2, trained_model_filename_2, save_model_dir_2)\n",
    "                print(f'Saved super_trained_model_{current_time}.json')\n",
    "    \n",
    "        finally:\n",
    "            sys.stdout = tee.console\n",
    "            tee.close()\n",
    "\n",
    "    print(f'Ending with epsilon = {epsilon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa235-adc5-4d0b-a1fe-cab8fd789e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22580f9d-bfe4-48f9-80d1-efe1409e640c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
