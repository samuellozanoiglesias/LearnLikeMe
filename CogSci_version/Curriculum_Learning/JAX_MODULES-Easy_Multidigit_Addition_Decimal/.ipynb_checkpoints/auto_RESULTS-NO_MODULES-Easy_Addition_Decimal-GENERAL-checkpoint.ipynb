{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98558e0d-b53a-412c-81a9-9bb54f749c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from tensorflow.keras.models import Model, load_model, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, Add, Lambda, Concatenate, Reshape, Flatten\n",
    "from tensorflow.keras.initializers import GlorotUniform, RandomUniform, Constant\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15cb903-d121-4c70-952d-9f60a5c352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/OneDrive - Universidad Complutense de Madrid (UCM)/Doctorado/Curriculum_Learning/JAX_MODULES-Easy_Multidigit_Addition_Decimal/'\n",
    "folder_specific = f'{folder}NO_MODULES/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5220b5d7-767d-4616-9750-922b01af448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow no está utilizando la GPU\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay GPUs disponibles\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow está utilizando la GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow no está utilizando la GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393fdea2-6341-4942-b3c1-a2b23b6dd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase que define la estructura del modelo\n",
    "class carry_LSTMModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        lstm_1 = nn.LSTMCell(features=16)\n",
    "        dense = nn.Dense(2)\n",
    "\n",
    "        carry1 = lstm_1.initialize_carry(jax.random.PRNGKey(0), (x.shape[0],)) \n",
    "\n",
    "        for t in range(x.shape[1]):  # Iterar sobre los pasos temporales\n",
    "            carry1, x_t = lstm_1(carry1, x[:, t])\n",
    "\n",
    "        hidden_state = carry1[0] \n",
    "        final_output = nn.softmax(dense(hidden_state))\n",
    "        return final_output\n",
    "\n",
    "class unit_LSTMModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        lstm_1 = nn.LSTMCell(features=16)\n",
    "        lstm_2 = nn.LSTMCell(features=32)\n",
    "        lstm_3 = nn.LSTMCell(features=16)\n",
    "        dense = nn.Dense(10)\n",
    "\n",
    "        carry1 = lstm_1.initialize_carry(jax.random.PRNGKey(0), (x.shape[0],))\n",
    "        carry2 = lstm_2.initialize_carry(jax.random.PRNGKey(1), (x.shape[0],))\n",
    "        carry3 = lstm_3.initialize_carry(jax.random.PRNGKey(2), (x.shape[0],))\n",
    "\n",
    "        for t in range(x.shape[1]):  # Iterar sobre los pasos temporales\n",
    "            carry1, x_t = lstm_1(carry1, x[:, t])\n",
    "            carry2, x_t = lstm_2(carry2, x_t)\n",
    "            carry3, x_t = lstm_3(carry3, x_t)\n",
    "\n",
    "        hidden_state = carry3[0]  # Estado oculto tiene forma (batch_size, 32)\n",
    "        final_output = nn.softmax(dense(hidden_state))\n",
    "        return final_output\n",
    "\n",
    "\n",
    "# Crear el estado inicial del modelo cargado (sin entrenar)\n",
    "def load_train_state(rng, learning_rate, initial_params, model):\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=initial_params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e640dcb3-ef42-4589-8409-d946e4f66dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las parejas desde el archivo\n",
    "with open(f\"{folder}train_couples_stimuli.txt\", \"r\") as file:\n",
    "    train_couples = eval(file.read())\n",
    "\n",
    "with open(f\"{folder}stimuli.txt\", \"r\") as file:\n",
    "    test_couples = eval(file.read())\n",
    "\n",
    "with open(f\"{folder}test_dataset.txt\", \"r\") as file:\n",
    "    test_dataset = eval(file.read())\n",
    "\n",
    "# Calcular frecuencias basadas en exp(-(a+b)/N) y normalizar\n",
    "probabilities = np.array([np.exp(-(a + b) / 100) for a, b in train_couples])\n",
    "probabilities /= probabilities.sum()  # Normalizar para convertir en una distribución de probabilidad\n",
    "\n",
    "def generate_test_dataset():\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for a, b in test_dataset:\n",
    "        a_dec = a // 10  # Decena del primer número\n",
    "        a_unit = a % 10  # Unidad del primer número\n",
    "        b_dec = b // 10  # Decena del segundo número\n",
    "        b_unit = b % 10  # Unidad del segundo número\n",
    "\n",
    "        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "\n",
    "        sum_units = (a_unit + b_unit) % 10\n",
    "        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "        y_data.append([sum_dec, sum_units])  # Salida\n",
    "    \n",
    "    return jnp.array(x_data), jnp.array(y_data)\n",
    "\n",
    "# Función para generar el dataset de entrenamiento dinámicamente\n",
    "def generate_train_dataset(train_couples, size_epoch):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # Seleccionar parejas aleatoriamente según las probabilidades\n",
    "    selected_indices = np.random.choice(len(train_couples), size=size_epoch, p=probabilities)\n",
    "    selected_couples = [train_couples[i] for i in selected_indices]\n",
    "\n",
    "    for a, b in selected_couples:\n",
    "        a_dec = a // 10  # Decena del primer número\n",
    "        a_unit = a % 10  # Unidad del primer número\n",
    "        b_dec = b // 10  # Decena del segundo número\n",
    "        b_unit = b % 10  # Unidad del segundo número\n",
    "\n",
    "        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "\n",
    "        sum_units = (a_unit + b_unit) % 10\n",
    "        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "        y_data.append([sum_dec, sum_units])  # Salida\n",
    "    \n",
    "    return jnp.array(x_data), jnp.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92293af0-33a3-408d-98e3-a37a680a8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de pérdida\n",
    "def loss_fn(params, x, y):\n",
    "    y_pred_1, y_pred_2 = model(params, x)\n",
    "    return jnp.mean((y_pred_1 - y[:, 0]) ** 2) + jnp.mean((y_pred_2 - y[:, 1]) ** 2)\n",
    "    \n",
    "# Función para actualizar los parámetros\n",
    "def update_params(params, x, y, lr):\n",
    "    # Asegúrate de usar JAX para los gradientes y operaciones\n",
    "    gradients = grad(loss_fn)(params, x, y)\n",
    "    #for key, gradient in gradients.items():\n",
    "    #    print(f\"Gradiente para {key}: {gradient}\")\n",
    "    new_params = jax.tree_util.tree_map(lambda p, g: p - lr * g, params, gradients)\n",
    "    return new_params\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(params, train_couples, size_epoch, lr=0.01, epochs=100, stop=1, batch_size=0):\n",
    "    final_loss = 0\n",
    "\n",
    "    if batch_size == 0:\n",
    "        batch_size = size_epoch\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    for epoch in range(epochs): \n",
    "        x_train, y_train = generate_train_dataset(train_couples, size_epoch)\n",
    "        total_examples = x_train.shape[0]\n",
    "\n",
    "        batches_per_epoch = int(total_examples / batch_size)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            pred_count, pred_count_test, step_loss = correct_predictions_and_loss(params)  # Contamos las predicciones correctas\n",
    "            print(f\"Epoch {epoch}, Loss: {step_loss}, Correct predictions: {pred_count}, Correct predictions test: {pred_count_test}\")\n",
    "            \n",
    "        for batch in range(batches_per_epoch):         \n",
    "            x_batch = x_train[(batch * batch_size):((batch + 1) * batch_size)]\n",
    "            y_batch = y_train[(batch * batch_size):((batch + 1) * batch_size)]\n",
    "            params = update_params(params, x_batch, y_batch, lr)\n",
    "            \n",
    "        # Mostrar estadísticas cada 10 épocas\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            pred_count, pred_count_test, step_loss = correct_predictions_and_loss(params)  # Contamos las predicciones correctas\n",
    "            print(f\"Epoch {epoch}, Loss: {step_loss}, Correct predictions: {pred_count}, Correct predictions test: {pred_count_test}\")\n",
    "            \n",
    "            # Criterios de parada\n",
    "            if stop == 1 and pred_count_test == 192:\n",
    "                break\n",
    "            if stop == 0 and pred_count == 5050:\n",
    "                break\n",
    "            if step_loss >= 1000000:\n",
    "                break\n",
    "        \n",
    "    return params, step_loss\n",
    "\n",
    "def correct_predictions_and_loss(params):\n",
    "    x_test, y_test = generate_test_dataset()\n",
    "    pred_count = 0\n",
    "    pred_count_test = 0\n",
    "    total_examples = x_test.shape[0]\n",
    "    pred_tens, pred_units = model(params, x_test)   \n",
    "    loss = jnp.mean((pred_tens - y_test[:, 0]) ** 2) + jnp.mean((pred_units - y_test[:, 1]) ** 2)\n",
    "    for i in range(total_examples):\n",
    "        normalized_pred = [int(jnp.round(pred_tens[i].item())),\n",
    "                           int(jnp.round(pred_units[i].item()))]\n",
    "        \n",
    "        # Obtener los valores a y b de x_test\n",
    "        a = int(str(x_test[i, 0]) + str(x_test[i, 1]))\n",
    "        b = int(str(x_test[i, 2]) + str(x_test[i, 3]))\n",
    "        # Comparar las predicciones con las etiquetas y contar los aciertos\n",
    "        if normalized_pred[0] == y_test[i, 0] and normalized_pred[1] == y_test[i, 1]:\n",
    "            pred_count += 1\n",
    "            if (a, b) in test_couples:\n",
    "                pred_count_test += 1\n",
    "\n",
    "    return pred_count, pred_count_test, loss\n",
    "\n",
    "def correct_predictions(params):\n",
    "    x_test, y_test = generate_test_dataset()\n",
    "    pred_count = 0\n",
    "    pred_count_test = 0\n",
    "    total_examples = x_test.shape[0]\n",
    "    pred_tens, pred_units = model(params, x_test)        \n",
    "    for i in range(total_examples):\n",
    "        normalized_pred = [int(jnp.round(pred_tens[i].item())),\n",
    "                           int(jnp.round(pred_units[i].item()))]\n",
    "        \n",
    "        # Obtener los valores a y b de x_test\n",
    "        a = int(str(x_test[i, 0]) + str(x_test[i, 1]))\n",
    "        b = int(str(x_test[i, 2]) + str(x_test[i, 3]))\n",
    "        # Comparar las predicciones con las etiquetas y contar los aciertos\n",
    "        if normalized_pred[0] == y_test[i, 0] and normalized_pred[1] == y_test[i, 1]:\n",
    "            pred_count += 1\n",
    "            if (a, b) in test_couples:\n",
    "                pred_count_test += 1\n",
    "\n",
    "    return pred_count, pred_count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8df9371-de6b-4e71-b765-5d6933ee3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_argmax_bivariate(x, k=10):\n",
    "    y = 1 / (1 + jnp.exp(-k * (x[:, 1] - x[:, 0])))\n",
    "    return y\n",
    "\n",
    "def smooth_argmax_multivariate(x):\n",
    "    smoothed_index = jnp.sum(x * jnp.arange(x.shape[1]), axis=-1)\n",
    "    return smoothed_index\n",
    "\n",
    "# Modelo dinámico en JAX\n",
    "def model(params, x):\n",
    "    carry_val = {}\n",
    "    unit_val = {}\n",
    "    decision_module_params = params['decision_params']\n",
    "    unit_module_params = params['unit_params'] \n",
    "    carry_module_params = params['carry_params'] \n",
    "    \n",
    "    for i in [0,1]:\n",
    "        for j in [2,3]:\n",
    "            units_inputs = jnp.array(x[:, [i, j]])\n",
    "            units_input = units_inputs[:, None, :]\n",
    "           \n",
    "            unit_output = unit_LSTMModel().apply({'params': unit_module_params}, units_input)\n",
    "            carry_output = carry_LSTMModel().apply({'params': carry_module_params}, units_input)\n",
    "           \n",
    "            unit_val[f'{i}_{j}'] = smooth_argmax_multivariate(unit_output)\n",
    "            carry_val[f'{i}_{j}'] = smooth_argmax_bivariate(carry_output)\n",
    "            #print(unit_val[f'0_2'])\n",
    "            #print(carry_val[f'0_2'])\n",
    "            \n",
    "    salida_1 = sum(decision_module_params[f'v_0_1_{i}_{j}'] * carry_val[f'{i}_{j}'] +\n",
    "        decision_module_params[f'v_1_1_{i}_{j}'] * unit_val[f'{i}_{j}']\n",
    "        for i in [0,1] for j in [2,3]\n",
    "        )\n",
    "\n",
    "    # Salida 2\n",
    "    salida_2 = sum(decision_module_params[f'v_0_2_{i}_{j}'] * carry_val[f'{i}_{j}'] +\n",
    "        decision_module_params[f'v_1_2_{i}_{j}'] * unit_val[f'{i}_{j}']\n",
    "        for i in [0,1] for j in [2,3]\n",
    "        )\n",
    "\n",
    "    return salida_1, salida_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bad68821-8b61-473b-ba5b-dba5eee4634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer parámetros de un archivo JSON\n",
    "def load_params_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "def save_trained_model(params, filename, model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    file_path = os.path.join(model_dir, filename)\n",
    "    serializable_params = convert_to_serializable(params)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(serializable_params, f)\n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, file, mode='w'):\n",
    "        self.file = open(file, mode)\n",
    "        self.console = sys.stdout  \n",
    "\n",
    "    def write(self, data):\n",
    "        self.console.write(data)   \n",
    "        self.file.write(data)    \n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "def convert_to_jnp_arrays(data):\n",
    "    \"\"\"Convierte listas y números en un diccionario en arreglos de JAX recursivamente.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Si el valor es un diccionario, aplica la conversión de forma recursiva\n",
    "        return {key: convert_to_jnp_arrays(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        # Si el valor es una lista, conviértela en un arreglo de JAX\n",
    "        return jnp.array(data)\n",
    "    elif isinstance(data, (int, float)):\n",
    "        # Si el valor es un número, conviértelo directamente en un arreglo de JAX\n",
    "        return jnp.array(data)\n",
    "    else:\n",
    "        # Deja otros tipos (como cadenas) sin cambios\n",
    "        return data\n",
    "\n",
    "def convert_to_serializable(data):\n",
    "    \"\"\"Convierte arreglos de JAX y listas en datos serializables recursivamente.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Si el valor es un diccionario, aplica la conversión recursiva\n",
    "        return {key: convert_to_serializable(value) for key, value in data.items()}\n",
    "    elif isinstance(data, jnp.ndarray):\n",
    "        # Convierte arreglos de JAX en listas\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, list):\n",
    "        # Convierte listas en datos serializables\n",
    "        return [convert_to_serializable(item) for item in data]\n",
    "    else:\n",
    "        # Deja otros tipos (como números o cadenas) sin cambios\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdb30455-12b9-41f9-9dfa-a993a1ac3b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with epsilon = 0.1\n",
      "Loaded trainable_model_2025_01_22_12_04_33.json\n",
      "Epoch 0, Loss: 49.755157470703125, Correct predictions: 0, Correct predictions test: 0\n",
      "Epoch 5, Loss: 13.910106658935547, Correct predictions: 67, Correct predictions test: 2\n",
      "Epoch 10, Loss: 13.902570724487305, Correct predictions: 68, Correct predictions test: 0\n",
      "Epoch 15, Loss: 13.659164428710938, Correct predictions: 68, Correct predictions test: 0\n",
      "Epoch 20, Loss: 13.532262802124023, Correct predictions: 65, Correct predictions test: 2\n",
      "Epoch 25, Loss: 13.506016731262207, Correct predictions: 65, Correct predictions test: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_type = 'AP'\n",
    "\n",
    "epsilons = [0.1]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    print(f'Starting with epsilon = {epsilon}')\n",
    "    save_dir = f\"{folder_specific}Results_models/{param_type}_{epsilon}\"\n",
    "    save_model_dir = f\"{folder_specific}Trained_models/{param_type}_{epsilon}\"\n",
    "    save_model_dir_2 = f\"{folder_specific}Super_trained_models/{param_type}_{epsilon}\"\n",
    "    folder_path = f'{folder_specific}Parameters/{param_type}_{epsilon}'\n",
    "    date_pattern = r'trainable_model_(\\d{4}_\\d{2}_\\d{2}_\\d{2}_\\d{2}_\\d{2}).json'\n",
    "    files = sorted(\n",
    "        (f for f in os.listdir(folder_path) if not f.startswith('.')),  # Filtrar archivos ocultos\n",
    "        key=lambda x: re.search(date_pattern, x).group(1) if re.search(date_pattern, x) else ''\n",
    "    )\n",
    "    \n",
    "    for filename in files:\n",
    "        match = re.search(date_pattern, filename)\n",
    "        if match:\n",
    "            current_time = match.group(1)\n",
    "        else:\n",
    "            print('Error')\n",
    "            break\n",
    "        \n",
    "        file_path = f\"{folder_path}/trainable_model_{current_time}.json\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            trainable_model = json.load(file)\n",
    "    \n",
    "        trainable_model_jnp = convert_to_jnp_arrays(trainable_model)\n",
    "        print(f'Loaded trainable_model_{current_time}.json')\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True) \n",
    "        results_file = os.path.join(save_dir, f\"Results_{current_time}.txt\") \n",
    "        tee = Tee(results_file, 'w') \n",
    "        sys.stdout = tee\n",
    "        \n",
    "        try: \n",
    "            new_params, average_loss = train_model(trainable_model_jnp, train_couples, size_epoch=1000, lr=0.01, epochs=100, stop=1, batch_size=0)\n",
    "            pred_count, pred_count_test = correct_predictions(new_params)\n",
    "\n",
    "            trained_model_filename = f\"trained_model_{current_time}.json\"\n",
    "            save_trained_model(new_params, trained_model_filename, save_model_dir)\n",
    "            print(f'Saved trained_model_{current_time}.json')\n",
    "    \n",
    "            if pred_count != 5051:\n",
    "                new_params_2, average_loss_2 = train_model(new_params, train_couples, size_epoch=1000, lr=0.01, epochs=500, stop=0, batch_size=0)\n",
    "                trained_model_filename_2 = f\"super_trained_model_{current_time}.json\"\n",
    "                save_trained_model(new_params_2, trained_model_filename_2, save_model_dir_2)\n",
    "                print(f'Saved super_trained_model_{current_time}.json')\n",
    "    \n",
    "        finally:\n",
    "            sys.stdout = tee.console\n",
    "            tee.close()\n",
    "\n",
    "    print(f'Ending with epsilon = {epsilon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa235-adc5-4d0b-a1fe-cab8fd789e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
