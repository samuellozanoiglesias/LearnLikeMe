{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98558e0d-b53a-412c-81a9-9bb54f749c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from tensorflow.keras.models import Model, load_model, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, Add, Lambda, Concatenate, Reshape, Flatten\n",
    "from tensorflow.keras.initializers import GlorotUniform, RandomUniform, Constant\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d15cb903-d121-4c70-952d-9f60a5c352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'D:/OneDrive - Universidad Complutense de Madrid (UCM)/Doctorado/Curriculum_Learning/Easy_Multidigit_Addition_Decimal/'\n",
    "folder_specific = f'{folder}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5220b5d7-767d-4616-9750-922b01af448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow no está utilizando la GPU\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay GPUs disponibles\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow está utilizando la GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow no está utilizando la GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1c8a971-a305-48fe-94a9-40be4a3f8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los módulos preentrenados (unit_module y carry_module)\n",
    "unit_addition_model = load_model(f'{folder}Modules/unit_addition_module.keras')\n",
    "unit_carry_model = load_model(f'{folder}Modules/unit_carry_module.keras')\n",
    "\n",
    "unit_addition_model.trainable = False\n",
    "unit_carry_model.trainable = False\n",
    "\n",
    "unit_addition_model.name = 'unit_addition_model'\n",
    "unit_carry_model.name = 'unit_carry_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e640dcb3-ef42-4589-8409-d946e4f66dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las parejas desde el archivo\n",
    "with open(f\"{folder}train_couples_stimuli.txt\", \"r\") as file:\n",
    "    train_couples = eval(file.read())\n",
    "\n",
    "with open(f\"{folder}stimuli.txt\", \"r\") as file:\n",
    "    test_couples = eval(file.read())\n",
    "\n",
    "with open(f\"{folder}test_dataset.txt\", \"r\") as file:\n",
    "    test_dataset = eval(file.read())\n",
    "\n",
    "small_problem_size = [pair for pair in train_couples if (pair[0] + pair[1]) < 40]\n",
    "medium_problem_size = [pair for pair in train_couples if 40 <= (pair[0] + pair[1]) <= 60]\n",
    "large_problem_size = [pair for pair in train_couples if 60 < (pair[0] + pair[1]) < 100]\n",
    "\n",
    "def generate_test_dataset():\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for a, b in test_dataset:\n",
    "        a_dec = a // 10  # Decena del primer número\n",
    "        a_unit = a % 10  # Unidad del primer número\n",
    "        b_dec = b // 10  # Decena del segundo número\n",
    "        b_unit = b % 10  # Unidad del segundo número\n",
    "\n",
    "        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "\n",
    "        sum_units = (a_unit + b_unit) % 10\n",
    "        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "        y_data.append([sum_dec, sum_units])  # Salida\n",
    "    \n",
    "    return jnp.array(x_data), jnp.array(y_data)\n",
    "\n",
    "# Función para generar el dataset de entrenamiento dinámicamente\n",
    "def generate_train_dataset(train_couples, size_epoch):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # Seleccionar parejas aleatoriamente según las probabilidades\n",
    "    selected_indices = np.random.choice(len(small_problem_size), size=size_epoch, replace=True)\n",
    "    selected_couples = [small_problem_size[i] for i in selected_indices]\n",
    "    \n",
    "    for a, b in selected_couples:\n",
    "        a_dec = a // 10  # Decena del primer número\n",
    "        a_unit = a % 10  # Unidad del primer número\n",
    "        b_dec = b // 10  # Decena del segundo número\n",
    "        b_unit = b % 10  # Unidad del segundo número\n",
    "\n",
    "        x_data.append([a_dec, a_unit, b_dec, b_unit])  # Entrada\n",
    "\n",
    "        sum_units = (a_unit + b_unit) % 10\n",
    "        carry_units = 1 if (a_unit + b_unit) >= 10 else 0\n",
    "        sum_dec = (a_dec + b_dec + carry_units) % 10\n",
    "        y_data.append([sum_dec, sum_units])  # Salida\n",
    "    print(x_data)\n",
    "    print(y_data)\n",
    "    \n",
    "    return jnp.array(x_data), jnp.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92293af0-33a3-408d-98e3-a37a680a8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de pérdida\n",
    "def loss_fn(params, x, y):\n",
    "    y_pred_1, y_pred_2 = model(params, x)\n",
    "    return jnp.mean((y_pred_1 - y[:, 0]) ** 2) + jnp.mean((y_pred_2 - y[:, 1]) ** 2)\n",
    "    \n",
    "# Función para actualizar los parámetros\n",
    "def update_params(params, x, y, lr):\n",
    "    # Asegúrate de usar JAX para los gradientes y operaciones\n",
    "    gradients = grad(loss_fn)(params, x, y)\n",
    "    #for key, gradient in gradients.items():\n",
    "    #    print(f\"Gradiente para {key}: {gradient}\")\n",
    "    new_params = jax.tree_util.tree_map(lambda p, g: p - lr * g, params, gradients)\n",
    "    return new_params\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(params, train_couples, size_epoch, lr=0.01, epochs=100, stop=1, batch_size=0):\n",
    "    final_loss = 0\n",
    "\n",
    "    if batch_size == 0:\n",
    "        batch_size = size_epoch\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    for epoch in range(epochs): \n",
    "        x_train, y_train = generate_train_dataset(train_couples, size_epoch)\n",
    "        total_examples = x_train.shape[0]\n",
    "\n",
    "        batches_per_epoch = int(total_examples / batch_size)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            pred_count, pred_count_test, step_loss = correct_predictions_and_loss(params)  # Contamos las predicciones correctas\n",
    "            print(f\"Epoch {epoch}, Loss: {step_loss}, Correct predictions: {pred_count}, Correct predictions test: {pred_count_test}\")\n",
    "            \n",
    "        for batch in range(batches_per_epoch):         \n",
    "            x_batch = x_train[(batch * batch_size):((batch + 1) * batch_size)]\n",
    "            y_batch = y_train[(batch * batch_size):((batch + 1) * batch_size)]\n",
    "            params = update_params(params, x_batch, y_batch, lr)\n",
    "            \n",
    "        # Mostrar estadísticas cada 10 épocas\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            pred_count, pred_count_test, step_loss = correct_predictions_and_loss(params)  # Contamos las predicciones correctas\n",
    "            print(f\"Epoch {epoch}, Loss: {step_loss}, Correct predictions: {pred_count}, Correct predictions test: {pred_count_test}\")\n",
    "            \n",
    "            # Criterios de parada\n",
    "            if stop == 1 and pred_count_test == 192:\n",
    "                break\n",
    "            if stop == 0 and pred_count == 5050:\n",
    "                break\n",
    "            if step_loss >= 1000000:\n",
    "                break\n",
    "        \n",
    "    return params, step_loss\n",
    "\n",
    "def correct_predictions_and_loss(params):\n",
    "    x_test, y_test = generate_test_dataset()\n",
    "    pred_count = 0\n",
    "    pred_count_test = 0\n",
    "    total_examples = x_test.shape[0]\n",
    "    pred_tens, pred_units = model(params, x_test)   \n",
    "    loss = jnp.mean((pred_tens - y_test[:, 0]) ** 2) + jnp.mean((pred_units - y_test[:, 1]) ** 2)\n",
    "    for i in range(total_examples):\n",
    "        normalized_pred = [int(jnp.round(pred_tens[i].item())),\n",
    "                           int(jnp.round(pred_units[i].item()))]\n",
    "        \n",
    "        # Obtener los valores a y b de x_test\n",
    "        a = int(str(x_test[i, 0]) + str(x_test[i, 1]))\n",
    "        b = int(str(x_test[i, 2]) + str(x_test[i, 3]))\n",
    "        # Comparar las predicciones con las etiquetas y contar los aciertos\n",
    "        if normalized_pred[0] == y_test[i, 0] and normalized_pred[1] == y_test[i, 1]:\n",
    "            pred_count += 1\n",
    "            if (a, b) in test_couples:\n",
    "                pred_count_test += 1\n",
    "\n",
    "    return pred_count, pred_count_test, loss\n",
    "\n",
    "def correct_predictions(params):\n",
    "    x_test, y_test = generate_test_dataset()\n",
    "    pred_count = 0\n",
    "    pred_count_test = 0\n",
    "    total_examples = x_test.shape[0]\n",
    "    pred_tens, pred_units = model(params, x_test)        \n",
    "    for i in range(total_examples):\n",
    "        normalized_pred = [int(jnp.round(pred_tens[i].item())),\n",
    "                           int(jnp.round(pred_units[i].item()))]\n",
    "        \n",
    "        # Obtener los valores a y b de x_test\n",
    "        a = int(str(x_test[i, 0]) + str(x_test[i, 1]))\n",
    "        b = int(str(x_test[i, 2]) + str(x_test[i, 3]))\n",
    "        # Comparar las predicciones con las etiquetas y contar los aciertos\n",
    "        if normalized_pred[0] == y_test[i, 0] and normalized_pred[1] == y_test[i, 1]:\n",
    "            pred_count += 1\n",
    "            if (a, b) in test_couples:\n",
    "                pred_count_test += 1\n",
    "\n",
    "    return pred_count, pred_count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d8df9371-de6b-4e71-b765-5d6933ee3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo dinámico en JAX\n",
    "def model(params, x):\n",
    "    carry_unit_val = {}\n",
    "    unit_val = {}\n",
    "\n",
    "    for i in [0,1]:\n",
    "       for j in [2,3]:\n",
    "            units_inputs = jnp.array(x[:, [i, j]])\n",
    "            units_input = units_inputs[:, None, :]\n",
    "           \n",
    "            unit_output = jnp.array(unit_addition_model(units_input))\n",
    "            unit_carry_output = jnp.array(unit_carry_model(units_input))\n",
    "           \n",
    "            unit_val[f'{i}_{j}'] = jnp.argmax(unit_output, axis=-1)\n",
    "            carry_unit_val[f'{i}_{j}'] = jnp.argmax(unit_carry_output, axis=-1)\n",
    "    \n",
    "    salida_1 = sum(params[f'v_0_1_{i}_{j}'] * carry_unit_val[f'{i}_{j}'] +\n",
    "        params[f'v_1_1_{i}_{j}'] * unit_val[f'{i}_{j}']\n",
    "        for i in [0,1] for j in [2,3]\n",
    "        )\n",
    "\n",
    "    # Salida 2\n",
    "    salida_2 = sum(params[f'v_0_2_{i}_{j}'] * carry_unit_val[f'{i}_{j}'] +\n",
    "        params[f'v_1_2_{i}_{j}'] * unit_val[f'{i}_{j}']\n",
    "        for i in [0,1] for j in [2,3]\n",
    "        )\n",
    "\n",
    "    return salida_1, salida_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bad68821-8b61-473b-ba5b-dba5eee4634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer parámetros de un archivo JSON\n",
    "def load_params_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "def save_trained_model(params, filename, model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    file_path = os.path.join(model_dir, filename)\n",
    "    serializable_params = {key: value.tolist() for key, value in params.items()}\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(serializable_params, f)\n",
    "\n",
    "class Tee(object):\n",
    "    def __init__(self, file, mode='w'):\n",
    "        self.file = open(file, mode)\n",
    "        self.console = sys.stdout  \n",
    "\n",
    "    def write(self, data):\n",
    "        self.console.write(data)   \n",
    "        self.file.write(data)    \n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fdb30455-12b9-41f9-9dfa-a993a1ac3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with epsilon = 0.001\n",
      "Loaded trainable_model_2025_01_21_00_50_40.json\n",
      "{'v_0_1_0_2': Array(-0.00274761, dtype=float32, weak_type=True), 'v_0_1_0_3': Array(0.0037168, dtype=float32, weak_type=True), 'v_0_1_1_2': Array(0.00331213, dtype=float32, weak_type=True), 'v_0_1_1_3': Array(0.9978564, dtype=float32, weak_type=True), 'v_0_2_0_2': Array(-0.00627775, dtype=float32, weak_type=True), 'v_0_2_0_3': Array(0.00678113, dtype=float32, weak_type=True), 'v_0_2_1_2': Array(0.0067669, dtype=float32, weak_type=True), 'v_0_2_1_3': Array(0.00954004, dtype=float32, weak_type=True), 'v_1_1_0_2': Array(0.99894416, dtype=float32, weak_type=True), 'v_1_1_0_3': Array(-0.00497116, dtype=float32, weak_type=True), 'v_1_1_1_2': Array(-0.00179951, dtype=float32, weak_type=True), 'v_1_1_1_3': Array(0.00197641, dtype=float32, weak_type=True), 'v_1_2_0_2': Array(0.0066047, dtype=float32, weak_type=True), 'v_1_2_0_3': Array(-0.00735645, dtype=float32, weak_type=True), 'v_1_2_1_2': Array(0.00062058, dtype=float32, weak_type=True), 'v_1_2_1_3': Array(0.99526703, dtype=float32, weak_type=True)}\n",
      "[[1, 4, 2, 0]]\n",
      "[[3, 4]]\n",
      "Epoch 0, Loss: 0.0021004085429012775, Correct predictions: 5050, Correct predictions test: 192\n",
      "[[1, 1, 0, 7]]\n",
      "[[1, 8]]\n",
      "[[0, 9, 2, 1]]\n",
      "[[3, 0]]\n",
      "[[1, 8, 1, 9]]\n",
      "[[3, 7]]\n",
      "[[0, 0, 2, 9]]\n",
      "[[2, 9]]\n",
      "[[1, 5, 0, 2]]\n",
      "[[1, 7]]\n",
      "Epoch 5, Loss: 0.009566301479935646, Correct predictions: 5050, Correct predictions test: 192\n",
      "Saved trained_model_2025_01_21_00_50_40.json\n",
      "[[3, 0, 0, 7]]\n",
      "[[3, 7]]\n",
      "Epoch 0, Loss: 0.009566301479935646, Correct predictions: 5050, Correct predictions test: 192\n",
      "[[0, 3, 3, 3]]\n",
      "[[3, 6]]\n",
      "[[2, 1, 1, 2]]\n",
      "[[3, 3]]\n",
      "[[0, 9, 2, 0]]\n",
      "[[2, 9]]\n",
      "[[0, 0, 1, 0]]\n",
      "[[1, 0]]\n",
      "[[1, 2, 0, 8]]\n",
      "[[2, 0]]\n",
      "Epoch 5, Loss: 0.00937697384506464, Correct predictions: 5050, Correct predictions test: 192\n",
      "Saved super_trained_model_2025_01_21_00_50_40.json\n",
      "Ending with epsilon = 0.001\n"
     ]
    }
   ],
   "source": [
    "param_type = 'AP'\n",
    "\n",
    "epsilons = [0.001]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    print(f'Starting with epsilon = {epsilon}')\n",
    "    save_dir = f\"{folder_specific}Results_models/{param_type}_{epsilon}\"\n",
    "    save_model_dir = f\"{folder_specific}Trained_models/{param_type}_{epsilon}\"\n",
    "    save_model_dir_2 = f\"{folder_specific}Super_trained_models/{param_type}_{epsilon}\"\n",
    "    folder_path = f'{folder_specific}Parameters/{param_type}_{epsilon}'\n",
    "    date_pattern = r'trainable_model_(\\d{4}_\\d{2}_\\d{2}_\\d{2}_\\d{2}_\\d{2}).json'\n",
    "    files = sorted(\n",
    "        (f for f in os.listdir(folder_path) if not f.startswith('.')),  # Filtrar archivos ocultos\n",
    "        key=lambda x: re.search(date_pattern, x).group(1) if re.search(date_pattern, x) else ''\n",
    "    )\n",
    "    \n",
    "    for filename in files:\n",
    "        match = re.search(date_pattern, filename)\n",
    "        if match:\n",
    "            current_time = match.group(1)\n",
    "        else:\n",
    "            print('Error')\n",
    "            break\n",
    "        \n",
    "        file_path = f\"{folder_path}/trainable_model_{current_time}.json\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            trainable_model = json.load(file)\n",
    "    \n",
    "        trainable_model_jnp = {key: jnp.array(value) for key, value in trainable_model.items()}\n",
    "        print(f'Loaded trainable_model_{current_time}.json')\n",
    "\n",
    "        print(trainable_model_jnp)\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True) \n",
    "        results_file = os.path.join(save_dir, f\"Results_{current_time}.txt\") \n",
    "        tee = Tee(results_file, 'w') \n",
    "        sys.stdout = tee\n",
    "        \n",
    "        try: \n",
    "            new_params, average_loss = train_model(trainable_model_jnp, train_couples, size_epoch=1, lr=0.01, epochs=100, stop=1, batch_size=0)\n",
    "            pred_count, pred_count_test = correct_predictions(new_params)\n",
    "\n",
    "            trained_model_filename = f\"trained_model_{current_time}.json\"\n",
    "            save_trained_model(new_params, trained_model_filename, save_model_dir)\n",
    "            print(f'Saved trained_model_{current_time}.json')\n",
    "    \n",
    "            if pred_count != 5051:\n",
    "                new_params_2, average_loss_2 = train_model(new_params, train_couples, size_epoch=1, lr=0.01, epochs=500, stop=0, batch_size=0)\n",
    "                trained_model_filename_2 = f\"super_trained_model_{current_time}.json\"\n",
    "                save_trained_model(new_params_2, trained_model_filename_2, save_model_dir_2)\n",
    "                print(f'Saved super_trained_model_{current_time}.json')\n",
    "    \n",
    "        finally:\n",
    "            sys.stdout = tee.console\n",
    "            tee.close()\n",
    "\n",
    "    print(f'Ending with epsilon = {epsilon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa235-adc5-4d0b-a1fe-cab8fd789e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
